# EVENTS ---------------------------------------------------------------------------------------------------------------
"""Volume-Price Events
1. Volume-Price Divergence Events

Price makes new highs/lows but volume doesn't confirm
More reliable than pure price events because volume shows conviction
Less noisy than Bollinger bands in sideways markets

2. Volume Shock Events

Volume spikes beyond 2-3 standard deviations of rolling average
Often precedes significant price moves
Can be combined with price acceleration for confirmation

Volatility Regime Events
3. Volatility Breakout Events

When realized volatility exceeds implied volatility by significant margin
Or when volatility moves beyond historical percentiles (95th/5th)
More fundamental than technical indicators

4. Volatility Clustering Events

Periods where volatility stays elevated for multiple periods
Based on GARCH-type modeling rather than simple rolling windows

Market Structure Events
5. Microstructure Breakdowns

When bid-ask spreads widen significantly (if you have L2 data)
Or when price gaps exceed normal distributions
Indicates liquidity stress or information asymmetry

6. Momentum Regime Changes

When short-term momentum (1-6 hours) diverges from medium-term (1-7 days)
Uses return autocorrelation changes rather than price levels
More stable across different market conditions

Statistical Events
7. Return Distribution Outliers

Events when returns exceed historical z-scores by 2+ standard deviations
Can be applied to log returns, volatility-adjusted returns, or residuals from factor models
More theoretically grounded than arbitrary technical levels

8. Correlation Breakdown Events

When ETH/BTC correlation breaks down significantly
Or when cross-timeframe correlations (5min vs 4H) diverge
Indicates regime changes or unique ETH-specific events

Multi-Asset Events
9. Cross-Asset Momentum Divergence

When ETH momentum diverges from broader crypto market
Compare ETH performance vs Bitcoin, major altcoins, or crypto index
Captures ETH-specific alpha opportunities

10. Funding Rate Events (if available)

When perp funding rates reach extreme levels
Indicates positioning imbalances that often reverse
More fundamental than pure technical analysis

My Top 3 Recommendations:

Volume-Price Divergence + Volatility Breakout: Combines price action with volume confirmation and volatility context
Return Distribution Outliers: Statistically robust, adapts to changing market conditions automatically
Multi-timeframe Momentum Regime Changes: Captures structural shifts rather than noise

The key advantage of these over cusum-bollinger is they're:

Less prone to false signals in choppy markets
More adaptable to changing market regimes
Theoretically grounded in market microstructure principles
Less curve-fitted to historical data

Which of these resonates with your trading philosophy? I can implement whichever approach interests you most."""
# THRESHOLD ------------------------------------------------------------------------------------------------------------
"""The most important questions in systematic trading. Using just trade commission as your barrier threshold is
typically too narrow and leads to several problems:
Problems with Commission-Only Thresholds:

Noise Trading: You'll trigger on every tiny price move, most of which are just market noise
High Turnover: Excessive trading costs from frequent entries/exits
Poor Risk-Adjusted Returns: Many small wins get wiped out by occasional larger losses
Ignores Market Volatility: Same threshold in low-vol and high-vol periods makes no sense

Better Threshold Optimization Methods:
1. Volatility-Adjusted Thresholds
threshold = k * σ_t * √(holding_period)
Where k is optimized through backtesting (typically 0.5-2.0)

Adapts to current market conditions
Larger thresholds in volatile periods, smaller in calm periods

2. Information-Theoretic Approach

Set thresholds based on signal-to-noise ratio of your features
Higher thresholds when your predictive features are weak
Lower when features show strong predictive power

3. Sharpe Ratio Optimization

Run backtests across different threshold multipliers
Choose the combination that maximizes risk-adjusted returns
Typically involves testing k values from 0.25 to 3.0 in increments

4. Transaction Cost + Market Impact Model
optimal_threshold = trading_costs + α * volatility + β * illiquidity_measure

Includes bid-ask spreads, slippage, market impact
More realistic than just commission

5. Meta-Learning Approach (Advanced)

Use machine learning to predict optimal thresholds based on market regime features
Features: volatility, volume, time of day, market stress indicators
Adapts thresholds dynamically

My Recommended Approach:
Two-Stage Optimization:
Stage 1: Volatility-Based Foundation
profit_threshold = commission + k_profit * daily_volatility
stop_threshold = commission + k_stop * daily_volatility
Stage 2: Sharpe Optimization

Test k_profit values: [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
Test k_stop values: [0.5, 0.75, 1.0, 1.25, 1.5]
Test asymmetric ratios (profit ≠ stop)

Key Considerations:

Asymmetric Thresholds: Often optimal to have profit_target ≠ stop_loss
Time Decay: Consider adding time-based exits (your vertical barrier)
Market Regime Awareness: Different thresholds for trending vs ranging markets
Sample Size: Ensure enough events for statistical significance

Practical Implementation:

Start with k=1.0 (1x daily volatility) as baseline
Run walk-forward optimization over your historical data
Monitor out-of-sample performance carefully
Re-optimize quarterly or when Sharpe ratio degrades significantly

The optimal threshold will likely be 2-5x your commission costs in typical crypto market conditions,
but this varies significantly with market volatility and your holding period."""
# INDICATORS -----------------------------------------------------------------------------------------------------------
"""creating relative indicators (distance from EMA, ratios, differences) is excellent practice for ML models. This is much better than using raw price levels because:
Why Relative Indicators Are Better:

Stationarity: Raw prices are non-stationary, but price-EMA differences are more stationary
Scale Independence: Models don't get confused by absolute price levels (ETH at $1000 vs $4000)
Feature Stability: Relationships remain consistent across different market periods
Reduced Multicollinearity: Price-derived ratios are less correlated than raw prices

Your approach of Close - EMA is good, but you can make it even better.
Recommended Indicators for Lopez de Prado + Elder Framework:
1. Trend Identification (Elder's Screen 1 - Market Tide)
Multi-Timeframe Momentum Ratios:
python# Better than raw MACD - normalized by volatility
macd_ratio = (MACD_line - MACD_signal) / ATR
trend_strength = (Close - EMA_long) / (ATR * sqrt(lookback_days))
momentum_divergence = (price_momentum_short / price_momentum_long) - 1
Trend Persistence Measures:
python# Fits Lopez de Prado's emphasis on autocorrelation
trend_consistency = rolling_correlation(returns, lagged_returns)
directional_strength = abs(Close - EMA) / (High - Low)  # Trend vs noise ratio
2. Market Structure (Lopez de Prado Microstructure)
Volume-Price Relationships:
pythonvolume_price_trend = correlation(volume_changes, price_changes, window=20)
volume_efficiency = abs(price_change) / volume_normalized  # Price impact per unit volume
accumulation_distribution = cumsum((Close - Low) - (High - Close)) / (High - Low) * Volume
Volatility Structure:
python# Lopez de Prado's volatility clustering
vol_regime = current_vol / rolling_mean(vol, long_window)
vol_persistence = correlation(vol_t, vol_t-1, window=20)
realized_vs_implied = realized_vol / implied_vol  # If you have options data
3. Entry Timing (Elder's Screen 2 - Wave Direction)
Mean Reversion vs Momentum:
python# Better than raw RSI - volatility adjusted
rsi_normalized = (RSI - 50) / ATR_percentile
mean_reversion_strength = (Close - VWAP) / daily_range
momentum_vs_reversion = short_momentum / mean_reversion_indicator
Oscillator Divergences:
pythonprice_oscillator = (EMA_fast - EMA_slow) / ATR
volume_oscillator = (Volume_MA_short - Volume_MA_long) / Volume_MA_long
oscillator_divergence = price_oscillator - volume_oscillator
4. Regime Detection (Lopez de Prado Structural Breaks)
Market Regime Indicators:
python# Volatility regime changes
vol_breakout = (current_vol - vol_MA) / vol_std
correlation_breakdown = rolling_corr_btc_eth - long_term_corr_btc_eth
liquidity_stress = bid_ask_spread / price  # If available
Information Flow:
python# Lopez de Prado's information-driven events
surprise_volume = (current_volume - expected_volume) / volume_std
price_acceleration = second_derivative(log_prices)
information_ratio = abs(returns) / volume_normalized
5. Risk Management (Both Frameworks)
Dynamic Position Sizing Inputs:
python# Lopez de Prado's bet sizing
signal_confidence = abs(prediction_probability - 0.5) * 2
volatility_forecast = GARCH_volatility or exponential_weighted_vol
correlation_adjustment = 1 / (1 + avg_correlation_with_other_positions)
Specific Recommendations for Your Script:
Replace These:

Raw EMAs → EMA distance ratios: (Close - EMA) / ATR
Raw MACD → MACD efficiency: MACD / volatility
Raw RSI → RSI momentum: (RSI - 50) * momentum_direction

Add These Cross-Timeframe Features:
python# Elder's triple screen across your 30m/4H/1D structure
trend_alignment = sign(30m_trend) * sign(4H_trend) * sign(1D_trend)
momentum_cascade = (30m_momentum / 4H_momentum) - 1
volatility_cascade = 30m_vol / 4H_vol
Meta-Features (Lopez de Prado Style):
pythonfeature_importance_decay = exponential_weight(feature_age)
prediction_confidence = abs(ensemble_prediction - 0.5)
regime_stability = rolling_std(market_regime_indicator)
Implementation Priority:

Start with volatility-normalized indicators - biggest bang for buck
Add cross-timeframe momentum ratios - fits Elder perfectly
Include volume-price efficiency measures - crucial for crypto
Build regime detection features - Lopez de Prado's key insight

This approach will give you features that are:

Stationary for ML models
Theoretically grounded in both frameworks
Robust across market regimes
Computationally efficient"""

"""According to Marcos Lopez de Prado and as seen on Pradofun script, when a statistical event occurs
 in a timeline of historical data, a triple barrier is applied, vertical after 24 hours
  and two symmetrical or asymmetrical horizontal barriers (ptsl profit take stop loss) to note the movement of price
   for the next 24 hours in the data frame (A new column is added). If price crosses first the upper horizontal,
   event is noted as 1, meaning profitable. If  price crosses lower horizontal or vertical,
    event is noted as 0 (not proffitable).
    Those notes are used as y axis for the later training of
    binary classification models predicting similar real statistical events. Question 1,
    Is it possible to implement methods doing the same within indicators script or is it
    better a new script to be created, taking results of indicators and add the column? Some function taking by
    argument one of the events of the data frame produced by indicators script and note in the same row how price
    was moved in the future by adding a column? Question 2 how ptsl levels can be dynamically calculated and set?
    Question 3 How vertical barrier length can be researched or dynamically adjusted without overfitting? 
Question 1: Implementation Location
I recommend creating a separate script rather than integrating it into the indicators script. Here's why:
Advantages of a separate script:

Separation of concerns: Indicators script focuses on feature engineering, labeling script focuses on target generation
Modularity: You can easily swap different labeling methods or use the same indicators with different labeling strategies
Performance: Event detection and barrier calculations can be computationally expensive - keeping them separate allows for optimization
Flexibility: Different events might need different barrier configurations
Testing: Easier to test and validate labeling logic independently

The separate script would:

Take the enhanced dataframe from indicators script as input
For each detected event (where any_event == True), apply triple barrier method
Add columns like barrier_touched, barrier_type, holding_period, return_achieved
Return the same dataframe with additional labeling columns

Question 2: Dynamic PTSL Level Calculation
López de Prado suggests several approaches for dynamic barrier setting:
Volatility-based approaches:

Use ATR (Average True Range) multiplied by a factor (e.g., 2x ATR for both barriers)
Use realized volatility over a rolling window
Use your existing vol_realized or ATR_pct columns from indicators

Event-specific approaches:

Different barrier widths for different event types (VPD events might need wider barriers than momentum events)
Scale barriers based on event "strength" (e.g., higher volume z-scores get wider barriers)

Market regime approaches:

Wider barriers during high volatility periods
Narrower barriers during trending markets vs ranging markets
Use your vol_regime_ indicators to adjust barrier width

Adaptive approaches:

Start with initial barrier width and adjust based on recent barrier hit statistics
If too many events hit vertical barrier, widen horizontal barriers
If too many hit horizontal barriers quickly, narrow them

Question 3: Vertical Barrier Research and Dynamic Adjustment
Research approaches for optimal holding period:
Statistical methods:

Half-life analysis: Calculate how long it takes for autocorrelations to decay to 50%
Mean reversion timing: Analyze how long price dislocations typically take to revert
Information decay: Study how long your features remain predictive

Event-specific analysis:

Different event types may have different optimal holding periods
VPD events might resolve faster than momentum regime changes
Analyze historical data to see typical resolution times for each event type

Market microstructure considerations:

Intraday events might resolve within hours
Daily events might need 3-10 days
Consider market session boundaries (don't cross weekends for short-term events)

Dynamic adjustment without overfitting:
Walk-forward approach:

Use expanding window to calculate optimal barriers
Recalibrate barriers every N periods using only past data
Never use future information

Cross-validation by time:

Split data into multiple time periods
Test different barrier combinations on out-of-sample periods
Use ensemble of different barrier settings

Robustness testing:

Test barrier sensitivity across different market regimes
Ensure consistent performance across different volatility environments
Use bootstrapping to test stability of optimal parameters

Practical constraints:

Set minimum/maximum barrier widths based on trading costs
Consider liquidity constraints (wider barriers during low liquidity)
Account for overnight gaps and market closures

The key insight from López de Prado
is that barriers should be economically meaningful rather than statistically optimized.
They should reflect realistic trading constraints and market dynamics
rather than being fitted to maximize historical performance."""

# Labeling
"""Main Features
1. Barrier Modes:

Static: Fixed percentage barriers (e.g., 2% profit, 1.5% stop loss)
Dynamic: Volatility-based barriers using ATR or other volatility measures
Adaptive: Barriers that adjust based on recent performance (simplified implementation)

2. Event Selection:

Choose specific event types (vpd_volatility_event, outlier_event, etc.)
Use combined any_event column
Event-specific barrier multipliers (wider barriers for regime changes, tighter for outliers)

3. Dynamic Barrier Calculation:

Uses your existing volatility columns (ATR_pct, vol_realized)
Configurable multipliers for profit take and stop loss
Dynamic vertical barriers based on volatility (more volatile = longer holding)
Min/max holding period constraints

4. Processing Features:

Parallel processing for large datasets
Intrabar barrier detection using High/Low prices
Comprehensive error handling
Progress tracking with tqdm

Usage Examples
python# Basic usage with dynamic barriers
labeled_data, summary = create_labeled_dataset(enhanced_data)

# Static barriers for conservative approach
labeled_data, summary = create_labeled_dataset(
    enhanced_data,
    barrier_mode='static'
)

# Focus on specific events with custom config
custom_config = {
    'event_types': ['outlier_event', 'momentum_regime_event'],
    'dynamic_barriers': {
        'pt_atr_multiplier': 2.5,
        'sl_atr_multiplier': 2.0,
        'max_holding_days': 3
    }
}
labeled_data, summary = create_labeled_dataset(
    enhanced_data,
    barrier_mode='dynamic',
    custom_config=custom_config
)
Key Implementation Details
1. Barrier Touch Detection:

Uses High/Low data for intrabar detection (more accurate than close-only)
Handles simultaneous barrier touches correctly
First barrier touched wins (realistic trading behavior)

2. Dynamic Sizing:

Profit take = ATR × multiplier (default 2.0)
Stop loss = ATR × multiplier (default 1.5)
Vertical barrier = volatility-based periods with min/max constraints

3. Event-Specific Adjustments:

VPD events get wider barriers (higher uncertainty)
Outlier events get tighter barriers (expected quick reversion)
Regime changes get much wider barriers (structural shifts)

4. Output Columns Added:

triple_barrier_label: Binary target (1=profitable, 0=not profitable)
barrier_touched: Which barrier was hit ('profit_take', 'stop_loss', 'vertical')
barrier_return: Actual return achieved
holding_period_hours: Time until barrier touch
Optional detailed barrier level information

The script addresses all your questions:

Separate implementation - Takes indicators output and adds labeling columns
Dynamic PTSL calculation - Based on ATR, volatility, and event types
Flexible vertical barriers - Volatility-based with constraints to prevent overfitting

The system is modular and allows you to experiment with different barrier strategies while maintaining the core López de Prado methodology.
"""